{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Импорт данных**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U sentence-transformers\n","!pip install mlflow\n","!pip install optuna\n","!pip install ipywidgets\n","!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import math\n","import warnings\n","import logging\n","import torch\n","import mlflow\n","import optuna\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from optuna.visualization import plot_optimization_history"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T14:07:12.605349Z","iopub.status.busy":"2023-12-19T14:07:12.604873Z","iopub.status.idle":"2023-12-19T14:07:12.693415Z","shell.execute_reply":"2023-12-19T14:07:12.692663Z","shell.execute_reply.started":"2023-12-19T14:07:12.605321Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023/12/19 14:07:12 INFO mlflow.tracking.fluent: Experiment with name 'logging_sbert' does not exist. Creating a new experiment.\n"]}],"source":["RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","torch.manual_seed(RANDOM_STATE)\n","warnings.simplefilter(\"ignore\", UserWarning)\n","warnings.simplefilter(\"ignore\", RuntimeWarning)\n","pd.set_option('max_colwidth', 400)\n","mlflow.set_experiment(\"logging_sbert\")\n","logging.basicConfig(format='%(asctime)s - %(message)s',\n","                    datefmt='%Y-%m-%d %H:%M:%S',\n","                    level=logging.INFO,\n","                    handlers=[logging.StreamHandler()])\n","log_dir = './logs'\n","os.makedirs(log_dir, exist_ok=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T13:48:58.838438Z","iopub.status.busy":"2023-12-19T13:48:58.838076Z","iopub.status.idle":"2023-12-19T13:48:58.842895Z","shell.execute_reply":"2023-12-19T13:48:58.841761Z","shell.execute_reply.started":"2023-12-19T13:48:58.838401Z"},"trusted":true},"outputs":[],"source":["# data = pd.read_csv('/kaggle/input/resume-hh/hh_ru.csv', delimiter=';')\n","# data['resume'] = data.apply(lambda row: ' '.join(row[data.columns].astype(str)), axis=1)\n","# df_hh = data['resume']\n","# df_yandex = pd.read_csv('/kaggle/input/yandex-jobs/vacancies.csv', sep=',')['Raw text']\n","# df_t = pd.concat([df_hh[:625], df_yandex], axis=1, ignore_index=True).rename(columns={0: 'Resume Description', 1: 'Job Description'})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# df = pd.read_csv('/kaggle/input/annotated-resume/resume_job.csv', sep=',', encoding='utf-8')\n","# df.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["**Набор данных**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T14:07:12.695419Z","iopub.status.busy":"2023-12-19T14:07:12.695131Z","iopub.status.idle":"2023-12-19T14:07:14.215434Z","shell.execute_reply":"2023-12-19T14:07:14.214423Z","shell.execute_reply.started":"2023-12-19T14:07:12.695393Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Resume Description</th>\n","      <th>Job Description</th>\n","      <th>Match</th>\n","      <th>city_resume</th>\n","      <th>city_job</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Пол, возраст: Мужчина ,  28 лет , родился 29 сентября 1990; ЗП: 25000 руб.; Ищет работу на должность: Продавец-кассир;\\nГород, переезд, командировки: Одинцово , не готов к переезду , не готов к командировкам; Занятость: полная занятость;\\nГрафик: полный день; Опыт работы: Опыт работы 1 год 11 месяцев  Сентябрь 2008 — Июль  2010 1 год 11 месяцев ООО Эльдорадо продавец консультант Продажа,консул...</td>\n","      <td>Название: Продавец-кассир\\n, Цена: от 59 000 ₽\\n, Условия: {'Сфера деятельности': 'Продажи', 'График работы': 'Сменный', 'Частота выплат': 'Дважды в месяц', 'Опыт работы': 'Без опыта', 'Что получают работники': 'униформа, питание, подарки детям на праздники'}\\n, Расположение: Одинцово\\n Описание: Tрудoустpоиться в кoмпанию может каждый! Пpинимаeм грaждaн СНГ и РФ - опыт не имeeт знaчeния! Bcем...</td>\n","      <td>1</td>\n","      <td>Одинцово</td>\n","      <td>Одинцово</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                Resume Description  \\\n","0  Пол, возраст: Мужчина ,  28 лет , родился 29 сентября 1990; ЗП: 25000 руб.; Ищет работу на должность: Продавец-кассир;\\nГород, переезд, командировки: Одинцово , не готов к переезду , не готов к командировкам; Занятость: полная занятость;\\nГрафик: полный день; Опыт работы: Опыт работы 1 год 11 месяцев  Сентябрь 2008 — Июль  2010 1 год 11 месяцев ООО Эльдорадо продавец консультант Продажа,консул...   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                   Job Description  \\\n","0  Название: Продавец-кассир\\n, Цена: от 59 000 ₽\\n, Условия: {'Сфера деятельности': 'Продажи', 'График работы': 'Сменный', 'Частота выплат': 'Дважды в месяц', 'Опыт работы': 'Без опыта', 'Что получают работники': 'униформа, питание, подарки детям на праздники'}\\n, Расположение: Одинцово\\n Описание: Tрудoустpоиться в кoмпанию может каждый! Пpинимаeм грaждaн СНГ и РФ - опыт не имeeт знaчeния! Bcем...   \n","\n","   Match city_resume  city_job  \n","0      1    Одинцово  Одинцово  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_excel('/kaggle/input/input-dataset/resume_job_cities_fixed.xlsx')\n","df.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["**Тренировка**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df, val_df = train_test_split(df, test_size=0.1, random_state=RANDOM_STATE)\n","train_data = [\n","    InputExample(texts=[job_desc, resume_desc], label=float(match))\n","    for job_desc, resume_desc, match in zip(train_df['Job Description'], train_df['Resume Description'], train_df['Match'])\n","]\n","train_dataloader = DataLoader(train_data, shuffle=True, batch_size=2, pin_memory=True)\n","\n","def create_evaluator(val_df, evaluator_class, name, batch_size):\n","    return evaluator_class(\n","        sentences1=val_df['Job Description'].tolist(),\n","        sentences2=val_df['Resume Description'].tolist(),\n","        labels=val_df['Match'].tolist(),\n","        name=name,\n","        batch_size=batch_size,\n","        show_progress_bar=True,\n","        write_csv=True\n","    )\n","\n","def train_and_evaluate(model, train_dataloader, evaluator, trial, loss_name):\n","    model = model.to(device)\n","    loss_class = getattr(losses, loss_name)\n","    train_loss = loss_class(model=model) \n","    mlflow.end_run()\n","\n","    with mlflow.start_run(run_name=\"ml_sbert\"):\n","        lr = trial.suggest_float('lr', 1e-6, 1e-2, log=True)\n","        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)\n","        batch_size = trial.suggest_int('batch_size', 2, 32, log=True)\n","        epochs = 15\n","        warmup_steps = math.ceil(len(train_df) * 0.1)\n","        checkpoint_path = './checkpoints'\n","        mlflow.log_params({'lr': lr, 'weight_decay': weight_decay, 'batch_size': batch_size, 'loss_name': loss_name})\n","\n","        for epoch in range(epochs):\n","            model.fit(\n","                train_objectives=[(train_dataloader, train_loss)],\n","                evaluator=evaluator,\n","                optimizer_class=torch.optim.AdamW,\n","                epochs=epochs,\n","                warmup_steps=warmup_steps,\n","                optimizer_params={'lr': lr},\n","                weight_decay=weight_decay,\n","                show_progress_bar=True,\n","                save_best_model=True,\n","                use_amp=True,\n","                checkpoint_path=checkpoint_path,\n","                checkpoint_save_steps=500,\n","            )\n","            metrics = evaluator(model)\n","            print(metrics)\n","            torch.cuda.empty_cache()\n","\n","    return metrics\n","\n","def objective(trial):\n","    model_name = \"sberbank-ai/sbert_large_nlu_ru\"\n","    model = SentenceTransformer(model_name)\n","    loss_name = trial.suggest_categorical('loss_name', ['CosineSimilarityLoss', 'ContrastiveLoss'])\n","    evaluator = create_evaluator(val_df, evaluation.BinaryClassificationEvaluator, \"binary_classification_evaluation\", batch_size=32)\n","    metrics = train_and_evaluate(model, train_dataloader, evaluator, trial, loss_name)\n","    \n","    return metrics\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=50)\n","best_params = study.best_params\n","print(\"Лучшие гиперпараметры:\", best_params)\n","\n","mlflow.log_params(best_params)\n","print(\"Тренировка завершена!\")\n","\n","fig = optuna.visualization.matplotlib.plot_optimization_history(study)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**TEST**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_t = pd.read_csv('/kaggle/input/yandex-jobs/vacancies.csv', sep=',')['Raw text']\n","df_t = df_t.to_frame().rename(columns={'Raw text': 'Job Description'})"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T15:48:10.995447Z","iopub.status.busy":"2023-12-19T15:48:10.994819Z","iopub.status.idle":"2023-12-19T15:48:11.931404Z","shell.execute_reply":"2023-12-19T15:48:11.930339Z","shell.execute_reply.started":"2023-12-19T15:48:10.995416Z"},"trusted":true},"outputs":[],"source":["!ls /kaggle/working/checkpoints"]},{"cell_type":"markdown","metadata":{},"source":["Последний лучший чекпоинт"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["latest_checkpoint = max(os.listdir('/kaggle/working/checkpoints'), key=lambda x: int(x))\n","checkpoint_filename = f'/kaggle/working/checkpoints/{latest_checkpoint}'\n","print(checkpoint_filename)\n","# model.load_state_dict(torch.load(checkpoint_filename))"]},{"cell_type":"markdown","metadata":{},"source":["Без резюме айтишников"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from scipy.spatial.distance import cosine\n","\n","best_model_path = \"/kaggle/working/checkpoints/1090\"\n","best_model = SentenceTransformer(best_model_path)\n","job_descriptions = df_t['Job Description'].tolist()\n","\n","user_resume = input('Введите своё резюме')\n","\n","user_resume_embedding = best_model.encode(user_resume)\n","similarity_scores = [1 - cosine(user_resume_embedding, best_model.encode(job_desc)) for job_desc in job_descriptions]\n","result_df = pd.DataFrame({'Job Description': job_descriptions, 'Similarity': similarity_scores})\n","top_10_vacancies = result_df.nlargest(10, 'Similarity')\n","print(\"Top-10 Vacancies for User's Resume:\")\n","print(top_10_vacancies[['Job Description', 'Similarity']])"]},{"cell_type":"markdown","metadata":{},"source":["С резюме айтишников"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_model_path = \"/kaggle/working/checkpoints/1090\"\n","best_model = SentenceTransformer(best_model_path)\n","\n","test_data = [\n","    InputExample(texts=[job_desc, resume_desc], label=float(match))\n","    for job_desc, resume_desc, match in zip(df_t['Job Description'], df_t['Resume Description'], df_t['Match'])\n","]\n","test_dataloader = DataLoader(test_data, shuffle=False, batch_size=2, pin_memory=True)\n","\n","test_evaluator = create_evaluator(df_t, evaluation.BinaryClassificationEvaluator, \"test_evaluation\", batch_size=32)\n","test_metrics = evaluator(model)\n","print(\"Test Metrics:\", test_metrics)\n","\n","user_resume = input('Введите своё резюме')\n","\n","new_data_embeddings = model.encode(df_t['Job Description'].tolist(), df_t['Resume Description'].tolist())\n","new_data['Similarity'] = model.encode([user_resume] * len(df_t), df_t['Job Description'])\n","top_10_new_data = new_data.nlargest(10, 'Similarity')\n","print(\"Top-10 Vacancies for New Data:\")\n","print(top_10_new_data[['Job Description', 'Similarity']])"]},{"cell_type":"markdown","metadata":{},"source":["**TRASH**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install langchain"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# with open('/kaggle/input/faiss-db/vector_db/db_jobs/index.pkl', 'rb') as f:\n","#     job_descriptions = pickle.load(f)\n","    \n","# with open('/kaggle/input/faiss-db/vector_db/db_resume/index.pkl', 'rb') as f:\n","#     resume = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import pickle\n","# import langchain\n","# from langchain.vectorstores import FAISS\n","# from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","\n","# # with open('/kaggle/input/faiss-db/vector_db/db_jobs/index.pkl', 'rb') as f:\n","# #     job_descriptions = pickle.load(f)\n","# embeddings = OpenAIEmbeddings()\n","# new_db = FAISS.load_local(\"/kaggle/input/faiss-db/vector_db/db_jobs/index.faiss\", embeddings)\n","# docs = new_db.similarity_search(query)\n","# docs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model = SentenceTransformer('/kaggle/working/checkpoints/best_model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# my_res = \"\"\"\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loaded_model = SentenceTransformer(\"fine_tuned_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# job_description_to_match = my_res\n","# job_description_embedding = model.encode(job_description_to_match, convert_to_tensor=True)\n","# resume_embeddings = model.encode(val_df['Job Description'].tolist(), convert_to_tensor=True)\n","\n","# similarities = util.pytorch_cos_sim(job_description_embedding, resume_embeddings)\n","# similarities = similarities.cpu().numpy()\n","\n","# top_matches_indices = np.argsort(similarities[0])[::-1][:5]\n","# top_matches_df = val_df.iloc[top_matches_indices][['Job Description', 'Match']]\n","\n","# print(\"\\nTop Matches for Resume:\")\n","# print(top_matches_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_matches_df.iloc[3, :]['Job Description']"]},{"cell_type":"markdown","metadata":{},"source":["**Tests**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n","# from torch.utils.data import Dataset, DataLoader\n","# from sklearn.model_selection import train_test_split\n","# from torch.utils.tensorboard import SummaryWriter\n","\n","# SEED = 42\n","# num_classes = 2\n","\n","# train_texts, val_texts, train_labels, val_labels = train_test_split(\n","#     df[['Job Description', 'Resume Description']].astype(str).values.tolist(),\n","#     df['Match'].tolist(),\n","#     test_size=0.2,\n","#     random_state=SEED\n","# )\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n","# model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n","\n","# def mean_pooling(model_output, attention_mask):\n","#     token_embeddings = model_output['last_hidden_state']\n","#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","#     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","#     sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","#     return sum_embeddings / sum_mask\n","\n","# class DownstreamTaskModel(torch.nn.Module):\n","#     def __init__(self, config, num_classes):\n","#         super(DownstreamTaskModel, self).__init__()\n","#         self.config = config\n","#         self.linear = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","#     def forward(self, input_ids, attention_mask):\n","#         model_output = model(input_ids, attention_mask=attention_mask)\n","#         sentence_embeddings = mean_pooling(model_output, attention_mask)\n","#         logits = self.linear(sentence_embeddings)\n","#         return logits\n","\n","# class DownstreamTaskDataset(Dataset):\n","#     def __init__(self, texts, labels, tokenizer, max_length=128):\n","#         self.texts = texts\n","#         self.labels = labels\n","#         self.tokenizer = tokenizer\n","#         self.max_length = max_length\n","\n","#         if len(self.texts) != len(self.labels):\n","#             raise ValueError(\"Length of texts and labels must be the same.\")\n","\n","#     def __len__(self):\n","#         return len(self.texts)\n","\n","#     def __getitem__(self, idx):\n","#         encoding = self.tokenizer(\n","#             self.texts[idx],\n","#             return_tensors='pt',\n","#             truncation=True,\n","#             max_length=self.max_length,\n","#             padding='max_length'\n","#         )\n","\n","#         label_ids = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","#         return {\n","#             'input_ids': encoding['input_ids'].flatten(),\n","#             'attention_mask': encoding['attention_mask'].flatten(),\n","#             'label_ids': label_ids\n","#         }\n","\n","# train_dataset = DownstreamTaskDataset(train_texts, train_labels, tokenizer)\n","# val_dataset = DownstreamTaskDataset(val_texts, val_labels, tokenizer)\n","\n","# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","# log_dir = './logs'\n","# os.makedirs(log_dir, exist_ok=True)\n","# writer = SummaryWriter(log_dir)\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","\n","# downstream_model = DownstreamTaskModel(model.config, num_classes)\n","# downstream_model.to(device)\n","\n","# optimizer = torch.optim.AdamW(downstream_model.parameters(), lr=5e-5)\n","# total_steps = len(train_loader) * 3\n","# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# criterion = torch.nn.CrossEntropyLoss()\n","\n","# for epoch in range(3):\n","#     downstream_model.train()\n","#     total_loss = 0.0\n","\n","#     for step, batch in enumerate(train_loader):\n","#         input_ids = batch['input_ids'].to(device)\n","#         attention_mask = batch['attention_mask'].to(device)\n","#         labels = batch['label_ids'].to(device)\n","\n","#         optimizer.zero_grad()\n","\n","#         logits = downstream_model(input_ids, attention_mask=attention_mask)\n","#         loss = criterion(logits, labels)\n","\n","#         total_loss += loss.item()\n","\n","#         loss.backward()\n","#         optimizer.step()\n","#         scheduler.step()\n","\n","#     avg_loss = total_loss / len(train_loader)\n","#     print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n","\n","# downstream_model.save_pretrained(\"fine_tuned_model\")\n","\n","# writer.close()\n","\n","# test_texts = df_t['Job Description'].astype(str).tolist() + df_t['Resume Description'].astype(str).tolist()\n","# test_dataset = DownstreamTaskDataset(test_texts, None, tokenizer)\n","# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# downstream_model.eval()\n","# all_embeddings = []\n","\n","# with torch.no_grad():\n","#     for batch in test_loader:\n","#         inputs = batch['input_ids'].to(device)\n","#         attention_mask = batch['attention_mask'].to(device)\n","\n","#         logits = downstream_model(inputs, attention_mask=attention_mask)\n","#         embeddings = mean_pooling({'last_hidden_state': logits}, attention_mask)\n","#         all_embeddings.append(embeddings)\n","\n","# all_embeddings = torch.cat(all_embeddings, dim=0)\n","# job_desc_embeddings, resume_embeddings = torch.chunk(all_embeddings, 2, dim=0)\n","# similarity_scores = cosine_similarity(job_desc_embeddings, resume_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import pandas as pd\n","# import torch\n","# from torch.utils.data import DataLoader\n","# from sklearn.model_selection import train_test_split\n","# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","# from transformers import DataCollatorForLanguageModeling\n","# from transformers import BertForSequenceClassification, BertConfig\n","\n","# num_labels = 2\n","# model_name = \"sismetanin/sbert-ru-sentiment-rusentiment\"\n","\n","# config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n","# new_classifier = torch.nn.Linear(config.hidden_size, num_labels)\n","# model = BertForSequenceClassification(config=config)\n","# model.classifier = new_classifier\n","\n","# # model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True, hidden_dropout_prob=0.1)\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# class JobMatchingDataset(torch.utils.data.Dataset):\n","#     def __init__(self, tokenizer, df, max_length=512):\n","#         self.tokenizer = tokenizer\n","#         self.df = df\n","#         self.max_length = max_length\n","\n","#     def __len__(self):\n","#         return len(self.df)\n","\n","#     def __getitem__(self, idx):\n","#         job_desc = self.df.iloc[idx]['Job Description']\n","#         resume_desc = self.df.iloc[idx]['Resume Description']\n","#         label = int(self.df.iloc[idx]['Match'])\n","\n","#         encoding = self.tokenizer(\n","#             job_desc,\n","#             resume_desc,\n","#             truncation=True,\n","#             padding=True,\n","#             max_length=self.max_length,\n","#             return_tensors='pt'\n","#         )\n","\n","#         return {\n","#             'input_ids': encoding['input_ids'].flatten(),\n","#             'attention_mask': encoding['attention_mask'].flatten(),\n","#             'labels': torch.tensor(label, dtype=torch.long)\n","#         }\n","\n","# class JobMatchingTestDataset(torch.utils.data.Dataset):\n","#     def __init__(self, tokenizer, df, max_length=512):\n","#         self.tokenizer = tokenizer\n","#         self.df = df\n","#         self.max_length = max_length\n","\n","#     def __len__(self):\n","#         return len(self.df)\n","\n","#     def __getitem__(self, idx):\n","#         job_desc = self.df.iloc[idx]['Job Description']\n","#         resume_desc = self.df.iloc[idx]['Resume Description']\n","\n","#         max_len_job_desc = len(self.tokenizer.encode(job_desc, max_length=None))\n","#         max_len_resume_desc = len(self.tokenizer.encode(resume_desc, max_length=None))\n","#         max_length = max(max_len_job_desc + max_len_resume_desc + self.max_length)\n","\n","#         encoding = self.tokenizer(\n","#             job_desc,\n","#             resume_desc,\n","#             truncation=True,\n","#             max_length=max_length,\n","#             return_tensors='pt'\n","#         )\n","\n","#         return {\n","#             'input_ids': encoding['input_ids'].flatten(),\n","#             'attention_mask': encoding['attention_mask'].flatten(),\n","#         }\n","\n","# train_dataset = JobMatchingDataset(tokenizer, df_train)\n","# val_dataset = JobMatchingDataset(tokenizer, df_val)\n","# test_dataset = JobMatchingTestDataset(tokenizer, df_t)\n","\n","# training_args = TrainingArguments(\n","#     output_dir=\"./job_matching_model\",\n","#     per_device_train_batch_size=2,\n","#     gradient_accumulation_steps=4,\n","#     per_device_eval_batch_size=8,\n","#     num_train_epochs=3,\n","#     logging_dir=\"./logs\",\n","#     logging_steps=100,\n","#     save_total_limit=2,\n","#     evaluation_strategy=\"steps\",\n","#     eval_steps=200,\n","# )\n","\n","# train_dataloader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)\n","# eval_dataloader = DataLoader(val_dataset, batch_size=training_args.per_device_eval_batch_size)\n","\n","# print(len(train_dataloader.dataset), len(eval_dataloader.dataset))\n","# print(training_args.per_device_train_batch_size, training_args.per_device_eval_batch_size)\n","\n","# if hasattr(training_args, \"gradient_accumulation_steps\"):\n","#     print(\"Gradient Accumulation Steps:\", training_args.gradient_accumulation_steps)\n","\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer=tokenizer,\n","#     mlm=False,\n","# )\n","\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=train_dataset,\n","#     eval_dataset=val_dataset,\n","#     data_collator=data_collator,\n","# )\n","\n","# trainer.train()\n","# results = trainer.evaluate()\n","\n","# model.save_pretrained(\"./job_matching_model\")\n","# tokenizer.save_pretrained(\"./job_matching_model\")\n","# predictions = []\n","\n","# for i in range(len(test_dataset)):\n","#     inputs = test_dataset[i]\n","#     max_length = inputs['input_ids'].size(1)\n","#     inputs = {k: torch.unsqueeze(v, 0) for k, v in inputs.items()}\n","#     outputs = model(**inputs)\n","#     predicted_label = torch.sigmoid(outputs.logits).squeeze().item()\n","#     predictions.append(predicted_label)\n","# df_t['Predicted_Match'] = predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import numpy as np\n","# import pandas as pd\n","# import torch\n","# from sentence_transformers import SentenceTransformer\n","# from sklearn.preprocessing import normalize\n","# from sklearn.metrics.pairwise import cosine_similarity\n","\n","# class JobMatchingTestDataset(torch.utils.data.Dataset):\n","#     def __init__(self, tokenizer, df, max_length=512):\n","#         self.tokenizer = tokenizer\n","#         self.df = df\n","#         self.max_length = max_length\n","\n","#     def __len__(self):\n","#         return len(self.df)\n","\n","#     def __getitem__(self, idx):\n","#         job_desc = self.df.iloc[idx]['Job Description']\n","#         resume_desc = self.df.iloc[idx]['Resume Description']\n","\n","#         encoding = self.tokenizer(\n","#             job_desc,\n","#             resume_desc,\n","#             truncation=True,\n","#             padding='max_length',\n","#             max_length=self.max_length,\n","#             return_tensors='pt'\n","#         )\n","\n","#         return {\n","#             'input_ids': encoding['input_ids'].flatten(),\n","#             'attention_mask': encoding['attention_mask'].flatten(),\n","#         }\n","\n","# test_dataset = JobMatchingTestDataset(tokenizer, df_t)\n","# model_name = \"sberbank-ai/sbert_large_nlu_ru\"\n","# sentence_transformer_model = SentenceTransformer(model_name)\n","\n","# max_length = 512\n","# job_tokenized = tokenizer(df['Job Description'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","# resume_tokenized = tokenizer(df['Resume Description'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","# test_job_tokenized = tokenizer(df_t['Job Description'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","# test_resume_tokenized = tokenizer(df_t['Resume Description'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","\n","# job_embeddings = sentence_transformer_model.encode(job_tokenized['input_ids'].tolist(), convert_to_tensor=True)\n","# resume_embeddings = sentence_transformer_model.encode(resume_tokenized['input_ids'].tolist(), convert_to_tensor=True)\n","# test_job_embeddings = sentence_transformer_model.encode(test_job_tokenized['input_ids'].tolist(), convert_to_tensor=True)\n","# test_resume_embeddings = sentence_transformer_model.encode(test_resume_tokenized['input_ids'].tolist(), convert_to_tensor=True)\n","\n","# job_embeddings = job_embeddings.cpu().numpy()\n","# resume_embeddings = resume_embeddings.cpu().numpy()\n","# test_job_embeddings = test_job_embeddings.cpu().numpy()\n","# test_resume_embeddings = test_resume_embeddings.cpu().numpy()\n","\n","# print(job_embeddings.shape, resume_embeddings.shape)\n","# print(type(job_embeddings), type(resume_embeddings))\n","# print(job_embeddings.shape, resume_embeddings.T.shape)\n","\n","# job_embeddings = normalize(job_embeddings)\n","# resume_embeddings = normalize(resume_embeddings)\n","\n","# print(\"Normalized Job Embeddings:\", job_embeddings)\n","# print(\"Normalized Resume Embeddings:\", resume_embeddings)\n","\n","# job_resume_similarities = cosine_similarity(job_embeddings, resume_embeddings)\n","# test_similarities = cosine_similarity(test_job_embeddings, test_resume_embeddings)\n","\n","# def find_top_matches(similarities, k=5):\n","#     top_matches = []\n","#     for i in range(len(similarities)):\n","#         top_k_indices = similarities[i].argsort()[-k:][::-1]\n","#         top_k_matches = df.iloc[top_k_indices][['Resume Description', 'Match']]\n","#         top_matches.append(top_k_matches)\n","#     return top_matches\n","\n","# top_matches_test = find_top_matches(test_similarities, k=5)\n","# print(\"\\nTop Matches for Test Data:\")\n","# for i, matches in enumerate(top_matches_test):\n","#     print(f\"Job Description {i+1}:\\n{matches}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for i, matches in enumerate(top_matches_test):\n","#     if i == 2:\n","#         break\n","#     print(f\"Job Description {i+1}:\\n{matches}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(\"Job Embeddings Shape:\", job_embeddings.shape)\n","# print(\"Resume Embeddings Shape:\", resume_embeddings.shape)\n","\n","# for i in range(5):\n","#     print(f\"\\nPair {i + 1} - Job Description:\\n{df.iloc[i]['Job Description']}\")\n","#     print(f\"Pair {i + 1} - Resume Description:\\n{df.iloc[i]['Resume Description']}\")\n","#     print(f\"Pair {i + 1} - Job Embeddings:\\n{job_embeddings[i]}\")\n","#     print(f\"Pair {i + 1} - Resume Embeddings:\\n{resume_embeddings[i]}\")\n","\n","# print(\"\\nNormalized Job Embeddings:\")\n","# print(job_embeddings)\n","# print(\"\\nNormalized Resume Embeddings:\")\n","# print(resume_embeddings)\n","\n","# job_resume_similarities = cosine_similarity(job_embeddings, resume_embeddings)\n","# test_similarities = cosine_similarity(test_job_embeddings, test_resume_embeddings)\n","\n","# def find_top_matches(similarities, k=5):\n","#     top_matches = []\n","#     for i in range(len(similarities)):\n","#         top_k_indices = similarities[i].argsort()[-k:][::-1]\n","#         top_k_matches = df.iloc[top_k_indices][['Resume Description', 'Match']]\n","#         top_matches.append(top_k_matches)\n","#     return top_matches\n","\n","# top_matches_test = find_top_matches(test_similarities, k=5)\n","# print(\"\\nTop Matches for Test Data:\")\n","# for i, matches in enumerate(top_matches_test):\n","#     print(f\"\\nJob Description {i + 1}:\\n{matches}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(\"Job Embeddings Shape:\", job_embeddings.shape)\n","# print(\"Resume Embeddings Shape:\", resume_embeddings.shape)\n","\n","# for i in range(5):\n","#     print(f\"\\nPair {i + 1} - Job Description:\\n{df.iloc[i]['Job Description']}\")\n","#     print(f\"Pair {i + 1} - Resume Description:\\n{df.iloc[i]['Resume Description']}\")\n","#     print(f\"Pair {i + 1} - Job Embeddings:\\n{job_embeddings[i]}\")\n","#     print(f\"Pair {i + 1} - Resume Embeddings:\\n{resume_embeddings[i]}\")\n","\n","# print(\"\\nNormalized Job Embeddings:\")\n","# print(job_embeddings)\n","# print(\"\\nNormalized Resume Embeddings:\")\n","# print(resume_embeddings)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2454054,"sourceId":4156699,"sourceType":"datasetVersion"},{"datasetId":3316555,"sourceId":5771012,"sourceType":"datasetVersion"},{"datasetId":4146683,"sourceId":7175818,"sourceType":"datasetVersion"},{"datasetId":4146803,"sourceId":7175978,"sourceType":"datasetVersion"},{"datasetId":4191833,"sourceId":7238086,"sourceType":"datasetVersion"},{"datasetId":4192254,"sourceId":7238621,"sourceType":"datasetVersion"}],"dockerImageVersionId":30615,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
